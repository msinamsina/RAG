{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845b19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41c8c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/RAG/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from langchain_core.documents import Document\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb6459",
   "metadata": {},
   "source": [
    "## Understanding Document Structur in Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd5c56e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Content: This is a sample document. This is a sample document. This is a sample document. This is a sample do...\n",
      "Document Metadata: {'source': 'sample_source.txt', 'author': 'John Doe', 'date': '2024-06-01', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# create a simple document\n",
    "doc = Document(\n",
    "    page_content=\"This is a sample document. \" * 100,\n",
    "    metadata={\n",
    "        \"source\": \"sample_source.txt\",\n",
    "        \"author\": \"John Doe\",\n",
    "        \"date\": \"2024-06-01\",\n",
    "        \"page\": 1}\n",
    ")\n",
    "\n",
    "print(\"Document Content:\", doc.page_content[:100] + \"...\")\n",
    "print(\"Document Metadata:\", doc.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733416e",
   "metadata": {},
   "source": [
    "### Text Files (.txt) - The Simplest Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f39991",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/texts\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d24c9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = {\n",
    "    \"data/texts/doc1.txt\": \"\"\"\n",
    "Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\"\n",
    "}\n",
    "\n",
    "for filepath, content in sample_texts.items():\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7835b0",
   "metadata": {},
   "source": [
    "### TextLoader - Read Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baff2a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s).\n",
      "[Document(metadata={'source': 'data/texts/doc1.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loader = TextLoader(\"data/texts/doc1.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} document(s).\")\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26021583",
   "metadata": {},
   "source": [
    "### DirectoryLoader - Multiple Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a1f71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 document(s) from directory.\n",
      "First document content: Python Programming Introduction\n",
      "\n",
      "Python is a high-level, interpreted programming language known for ...\n",
      "First document metadata: {'source': 'data/texts/doc1.txt'}\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"data/texts\", glob=\"*.txt\", loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s) from directory.\")\n",
    "print(\"First document content:\", documents[0].page_content[:100] + \"...\")\n",
    "print(\"First document metadata:\", documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755a97c",
   "metadata": {},
   "source": [
    "## Text Splitting Strategeis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bbda14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 634, which is longer than the specified 200\n",
      "Created a chunk of size 558, which is longer than the specified 200\n",
      "Created a chunk of size 693, which is longer than the specified 200\n",
      "Created a chunk of size 667, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-based splitting produced 6 chunks.\n",
      "Chunk 1:\n",
      "The Rise of Large Language Models: A New Era in Artificial Intelligence\n",
      "\n",
      "Chunk 2:\n",
      "The field of artificial intelligence (AI) has witnessed remarkable advancements over the past decade, with one of the most significant breakthroughs being the development of Large Language Models (LLMs). These models, such as OpenAI's GPT (Generative Pre-trained Transformer) series, Google's LaMDA and PaLM, and Meta's LLaMA, have demonstrated an extraordinary ability to understand, generate, and interact with human language in a way that was previously the realm of science fiction. Their impact is being felt across countless industries, from software development and content creation to customer service and scientific research.\n",
      "\n",
      "Chunk 3:\n",
      "The journey to modern LLMs began with foundational concepts in natural language processing (NLP) and machine learning. Early statistical models, like n-grams, laid the groundwork by predicting the next word in a sequence based on the previous ones. However, these models lacked a deep understanding of context. The introduction of neural networks, particularly Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, allowed models to maintain a 'memory' of previous inputs, improving their ability to handle longer dependencies in text.\n",
      "\n",
      "Chunk 4:\n",
      "A pivotal moment came with the introduction of the Transformer architecture in the 2017 paper \"Attention Is All You Need\" by researchers at Google. The Transformer's key innovation was the self-attention mechanism, which enabled the model to weigh the importance of different words in the input text, regardless of their position. This parallelizable architecture allowed for the training of much larger models on vast amounts of data, a crucial ingredient for the success of LLMs. The \"pre-training\" and \"fine-tuning\" paradigm emerged, where a model is first trained on a massive, general-purpose text corpus (like the entire internet) and then fine-tuned on a smaller, task-specific dataset.\n",
      "\n",
      "Chunk 5:\n",
      "The scaling hypothesis—the idea that performance improves predictably as model size, dataset size, and computational budget increase—has been a driving force. GPT-3, with its 175 billion parameters, was a landmark model that showcased \"in-context learning,\" the ability to perform tasks it wasn't explicitly trained on, simply by being given a few examples in the prompt. Subsequent models have grown even larger, incorporating multi-modal capabilities (processing images, audio, and video alongside text) and more sophisticated training techniques like Reinforcement Learning from Human Feedback (RLHF) to better align their outputs with human values and intentions.\n",
      "\n",
      "Chunk 6:\n",
      "The applications of LLMs are vast and continue to expand. They power chatbots and virtual assistants that are more conversational and helpful than ever before. In software engineering, they assist with code generation, debugging, and documentation (e.g., GitHub Copilot). Content creators use them to draft articles, marketing copy, and scripts. In the scientific community, they are being used to analyze research papers, generate hypotheses, and even help in drug discovery. However, the rise of LLMs also brings significant challenges and ethical considerations. Issues such as bias in training data, the potential for misuse in generating misinformation, environmental concerns due to the massive computational power required for training, and the impact on the job market are subjects of ongoing debate and research. As we move forward, harnessing the power of LLMs responsibly will be one of the most critical challenges of our time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# character-based splitting\n",
    "\n",
    "text = \"\"\"The Rise of Large Language Models: A New Era in Artificial Intelligence\n",
    "\n",
    "The field of artificial intelligence (AI) has witnessed remarkable advancements over the past decade, with one of the most significant breakthroughs being the development of Large Language Models (LLMs). These models, such as OpenAI's GPT (Generative Pre-trained Transformer) series, Google's LaMDA and PaLM, and Meta's LLaMA, have demonstrated an extraordinary ability to understand, generate, and interact with human language in a way that was previously the realm of science fiction. Their impact is being felt across countless industries, from software development and content creation to customer service and scientific research.\n",
    "\n",
    "The journey to modern LLMs began with foundational concepts in natural language processing (NLP) and machine learning. Early statistical models, like n-grams, laid the groundwork by predicting the next word in a sequence based on the previous ones. However, these models lacked a deep understanding of context. The introduction of neural networks, particularly Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, allowed models to maintain a 'memory' of previous inputs, improving their ability to handle longer dependencies in text.\n",
    "\n",
    "A pivotal moment came with the introduction of the Transformer architecture in the 2017 paper \"Attention Is All You Need\" by researchers at Google. The Transformer's key innovation was the self-attention mechanism, which enabled the model to weigh the importance of different words in the input text, regardless of their position. This parallelizable architecture allowed for the training of much larger models on vast amounts of data, a crucial ingredient for the success of LLMs. The \"pre-training\" and \"fine-tuning\" paradigm emerged, where a model is first trained on a massive, general-purpose text corpus (like the entire internet) and then fine-tuned on a smaller, task-specific dataset.\n",
    "\n",
    "The scaling hypothesis—the idea that performance improves predictably as model size, dataset size, and computational budget increase—has been a driving force. GPT-3, with its 175 billion parameters, was a landmark model that showcased \"in-context learning,\" the ability to perform tasks it wasn't explicitly trained on, simply by being given a few examples in the prompt. Subsequent models have grown even larger, incorporating multi-modal capabilities (processing images, audio, and video alongside text) and more sophisticated training techniques like Reinforcement Learning from Human Feedback (RLHF) to better align their outputs with human values and intentions.\n",
    "\n",
    "The applications of LLMs are vast and continue to expand. They power chatbots and virtual assistants that are more conversational and helpful than ever before. In software engineering, they assist with code generation, debugging, and documentation (e.g., GitHub Copilot). Content creators use them to draft articles, marketing copy, and scripts. In the scientific community, they are being used to analyze research papers, generate hypotheses, and even help in drug discovery. However, the rise of LLMs also brings significant challenges and ethical considerations. Issues such as bias in training data, the potential for misuse in generating misinformation, environmental concerns due to the massive computational power required for training, and the impact on the job market are subjects of ongoing debate and research. As we move forward, harnessing the power of LLMs responsibly will be one of the most critical challenges of our time.\"\"\"\n",
    "\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "\n",
    "char_chunks = char_splitter.split_text(text)\n",
    "print(f\"Character-based splitting produced {len(char_chunks)} chunks.\")\n",
    "for i, chunk in enumerate(char_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bed2410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive character-based splitting produced 22 chunks.\n",
      "Chunk 1:\n",
      "The Rise of Large Language Models: A New Era in Artificial Intelligence\n",
      "\n",
      "Chunk 2:\n",
      "The field of artificial intelligence (AI) has witnessed remarkable advancements over the past decade, with one of the most significant breakthroughs being the development of Large Language Models\n",
      "\n",
      "Chunk 3:\n",
      "Language Models (LLMs). These models, such as OpenAI's GPT (Generative Pre-trained Transformer) series, Google's LaMDA and PaLM, and Meta's LLaMA, have demonstrated an extraordinary ability to\n",
      "\n",
      "Chunk 4:\n",
      "ability to understand, generate, and interact with human language in a way that was previously the realm of science fiction. Their impact is being felt across countless industries, from software\n",
      "\n",
      "Chunk 5:\n",
      "from software development and content creation to customer service and scientific research.\n",
      "\n",
      "Chunk 6:\n",
      "The journey to modern LLMs began with foundational concepts in natural language processing (NLP) and machine learning. Early statistical models, like n-grams, laid the groundwork by predicting the\n",
      "\n",
      "Chunk 7:\n",
      "by predicting the next word in a sequence based on the previous ones. However, these models lacked a deep understanding of context. The introduction of neural networks, particularly Recurrent Neural\n",
      "\n",
      "Chunk 8:\n",
      "Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, allowed models to maintain a 'memory' of previous inputs, improving their ability to handle longer dependencies in text.\n",
      "\n",
      "Chunk 9:\n",
      "A pivotal moment came with the introduction of the Transformer architecture in the 2017 paper \"Attention Is All You Need\" by researchers at Google. The Transformer's key innovation was the\n",
      "\n",
      "Chunk 10:\n",
      "innovation was the self-attention mechanism, which enabled the model to weigh the importance of different words in the input text, regardless of their position. This parallelizable architecture\n",
      "\n",
      "Chunk 11:\n",
      "architecture allowed for the training of much larger models on vast amounts of data, a crucial ingredient for the success of LLMs. The \"pre-training\" and \"fine-tuning\" paradigm emerged, where a model\n",
      "\n",
      "Chunk 12:\n",
      "where a model is first trained on a massive, general-purpose text corpus (like the entire internet) and then fine-tuned on a smaller, task-specific dataset.\n",
      "\n",
      "Chunk 13:\n",
      "The scaling hypothesis—the idea that performance improves predictably as model size, dataset size, and computational budget increase—has been a driving force. GPT-3, with its 175 billion parameters,\n",
      "\n",
      "Chunk 14:\n",
      "billion parameters, was a landmark model that showcased \"in-context learning,\" the ability to perform tasks it wasn't explicitly trained on, simply by being given a few examples in the prompt.\n",
      "\n",
      "Chunk 15:\n",
      "in the prompt. Subsequent models have grown even larger, incorporating multi-modal capabilities (processing images, audio, and video alongside text) and more sophisticated training techniques like\n",
      "\n",
      "Chunk 16:\n",
      "techniques like Reinforcement Learning from Human Feedback (RLHF) to better align their outputs with human values and intentions.\n",
      "\n",
      "Chunk 17:\n",
      "The applications of LLMs are vast and continue to expand. They power chatbots and virtual assistants that are more conversational and helpful than ever before. In software engineering, they assist\n",
      "\n",
      "Chunk 18:\n",
      "they assist with code generation, debugging, and documentation (e.g., GitHub Copilot). Content creators use them to draft articles, marketing copy, and scripts. In the scientific community, they are\n",
      "\n",
      "Chunk 19:\n",
      "community, they are being used to analyze research papers, generate hypotheses, and even help in drug discovery. However, the rise of LLMs also brings significant challenges and ethical\n",
      "\n",
      "Chunk 20:\n",
      "and ethical considerations. Issues such as bias in training data, the potential for misuse in generating misinformation, environmental concerns due to the massive computational power required for\n",
      "\n",
      "Chunk 21:\n",
      "power required for training, and the impact on the job market are subjects of ongoing debate and research. As we move forward, harnessing the power of LLMs responsibly will be one of the most\n",
      "\n",
      "Chunk 22:\n",
      "be one of the most critical challenges of our time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "recursive_chunks = recursive_splitter.split_text(text)\n",
    "print(f\"Recursive character-based splitting produced {len(recursive_chunks)} chunks.\")\n",
    "for i, chunk in enumerate(recursive_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4220321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-based splitting produced 4 chunks.\n",
      "Chunk 1:\n",
      "The Rise of Large Language Models: A New Era in Artificial Intelligence\n",
      "\n",
      "The field of artificial intelligence (AI) has witnessed remarkable advancements over the past decade, with one of the most significant breakthroughs being the development of Large Language Models (LLMs). These models, such as OpenAI's GPT (Generative Pre-trained Transformer) series, Google's LaMDA and PaLM, and Meta's LLaMA, have demonstrated an extraordinary ability to understand, generate, and interact with human language in a way that was previously the realm of science fiction. Their impact is being felt across countless industries, from software development and content creation to customer service and scientific research.\n",
      "\n",
      "The journey to modern LLMs began with foundational concepts in natural language processing (NLP) and machine learning. Early statistical models, like n-grams, laid the groundwork by predicting the next word in a sequence based on the previous ones. However, these models lacked a deep understanding of context. The\n",
      "\n",
      "Chunk 2:\n",
      " a sequence based on the previous ones. However, these models lacked a deep understanding of context. The introduction of neural networks, particularly Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, allowed models to maintain a 'memory' of previous inputs, improving their ability to handle longer dependencies in text.\n",
      "\n",
      "A pivotal moment came with the introduction of the Transformer architecture in the 2017 paper \"Attention Is All You Need\" by researchers at Google. The Transformer's key innovation was the self-attention mechanism, which enabled the model to weigh the importance of different words in the input text, regardless of their position. This parallelizable architecture allowed for the training of much larger models on vast amounts of data, a crucial ingredient for the success of LLMs. The \"pre-training\" and \"fine-tuning\" paradigm emerged, where a model is first trained on a massive, general-purpose text corpus (like the entire internet\n",
      "\n",
      "Chunk 3:\n",
      " where a model is first trained on a massive, general-purpose text corpus (like the entire internet) and then fine-tuned on a smaller, task-specific dataset.\n",
      "\n",
      "The scaling hypothesis—the idea that performance improves predictably as model size, dataset size, and computational budget increase—has been a driving force. GPT-3, with its 175 billion parameters, was a landmark model that showcased \"in-context learning,\" the ability to perform tasks it wasn't explicitly trained on, simply by being given a few examples in the prompt. Subsequent models have grown even larger, incorporating multi-modal capabilities (processing images, audio, and video alongside text) and more sophisticated training techniques like Reinforcement Learning from Human Feedback (RLHF) to better align their outputs with human values and intentions.\n",
      "\n",
      "The applications of LLMs are vast and continue to expand. They power chatbots and virtual assistants that are more conversational and helpful than ever before. In software engineering,\n",
      "\n",
      "Chunk 4:\n",
      " chatbots and virtual assistants that are more conversational and helpful than ever before. In software engineering, they assist with code generation, debugging, and documentation (e.g., GitHub Copilot). Content creators use them to draft articles, marketing copy, and scripts. In the scientific community, they are being used to analyze research papers, generate hypotheses, and even help in drug discovery. However, the rise of LLMs also brings significant challenges and ethical considerations. Issues such as bias in training data, the potential for misuse in generating misinformation, environmental concerns due to the massive computational power required for training, and the impact on the job market are subjects of ongoing debate and research. As we move forward, harnessing the power of LLMs responsibly will be one of the most critical challenges of our time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# token-based splitting\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(text)\n",
    "print(f\"Token-based splitting produced {len(token_chunks)} chunks.\")\n",
    "for i, chunk in enumerate(token_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21e318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
